{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfdbf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from decoder import VAE_AttentionBlock, VAE_ResidualBlock\n",
    "\n",
    "class VAE_Encoder(nn.Sequential):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (Batch_Size, Channel, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height/2, Width/2)\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (Batch_Size, 128, Height/2, Width/2) -> (Batch_Size, 256, Height/2, Width/2)\n",
    "            VAE_ResidualBlock(128, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height/2, Width/2) -> (Batch_Size, 256, Height/2, Width/2)\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height/2, Width/2) -> (Batch_Size, 512, Height/4, Width/4)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (Batch_Size, 256, Height/4, Width/4) -> (Batch_Size, 512, Height/4, Width/4)\n",
    "            VAE_ResidualBlock(256, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/4, Width/4) -> (Batch_Size, 512, Height/4, Width/4)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/4, Width/4) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            \n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            VAE_AttentionBlock(512),\n",
    "            \n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            nn.GroupNorm(32, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 512, Height/8, Width/8)\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (Batch_Size, 512, Height/8, Width/8) -> (Batch_Size, 8, Height/16, Width/16)\n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 8, Height/8, Width/8) -> (Batch_Size, 8, Height/16, Width/16)\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (Batch_Size, Channel, Height, Width)\n",
    "        # noise: (Batch_Size, Out_Channel, Height/8, Width/8)\n",
    "\n",
    "        # output of variational encoder is a Gaussian distribution parameterized by mean and log-variance\n",
    "        # but not the compressed input itself\n",
    "\n",
    "        for module in self:\n",
    "            if getattr(module, 'stride', None) == (2,2):\n",
    "                # (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom)\n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "\n",
    "            x = module(x)\n",
    "\n",
    "        # (Batch_Size, 8, Height/8, Width/8) -> (Batch_Size, 4, Height/8, Width/8), (Batch_Size, 4, Height/8, Width/8)\n",
    "        # chunk split input tensor into n tensors along dimension specified\n",
    "        mean, log_varaince = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # (Batch_Size, 4, Height/8, Width/8)\n",
    "        log_varaince = torch.clamp(log_varaince, min=-30.0, max=20.0)\n",
    "\n",
    "        variance = log_varaince.exp()\n",
    "\n",
    "        stdev = variance.sqrt()\n",
    "\n",
    "        # Z=N(0, 1) -> N(mean, variance)\n",
    "        x = mean + stdev * noise\n",
    "\n",
    "        # Scale the output by a constant factor\n",
    "        x = x * 0.18215\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from attention import SelfAttention\n",
    "\n",
    "class VAE_AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (Batch_Size, Channels, Height, Width)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "\n",
    "        # (Batch_Size, Channels, Height, Width) -> (Batch_Size, Height*Width, Channels)\n",
    "        x = x.view(n, c, h*w).transpose(-1, -2)\n",
    "\n",
    "        # (Batch_Size, Height*Width, Channels)\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # (Batch_Size, Height*Width, Channels) -> (Batch_Size, Channels, Height, Width)\n",
    "        x = x.transpose(-1, -2).view(n, c, h, w)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class VAE_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (Batch_Size, In_Channels, Height, Width)\n",
    "        residual = x\n",
    "\n",
    "        x = self.groupnorm_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.groupnorm_2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x + self.residual_layer(residual)\n",
    "\n",
    "\n",
    "class VAE_Decoder(nn.Sequential):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
    "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_AttentionBlock(512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            # (Batch_Size, 512, Height/8, Width/8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/4, Width/4)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height/2, Width/2)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            VAE_ResidualBlock(512, 256),\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height, Width)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "\n",
    "            VAE_ResidualBlock(256, 128),\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            nn.GroupNorm(32, 128),\n",
    "            nn.SiLU(),\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 3, Height, Width)\n",
    "            nn.Conv2d(128, 3, kernel_size=3, padding=1)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (Batch_Size, 4, Height/8, Width/8)\n",
    "        x /= 0.18215\n",
    "\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "\n",
    "        # (Batch_Size, 3, Height, Width)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1f43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdfb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_embed: int, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_proj = nn.Linear(d_embed, d_embed * 3, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, causal_mask=False) -> torch.Tensor:\n",
    "        # x: (Batch_Size, Seq_Len, D_Embed)\n",
    "\n",
    "        input_shape = x.shape\n",
    "\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "\n",
    "        intermim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, D_Embed/3)\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        q = q.view(intermim_shape).transpose(1, 2)  # (Batch_Size, N_Heads, Seq_Len, D_Head)\n",
    "        k = k.view(intermim_shape).transpose(1, 2)  # (Batch_Size, N_Heads, Seq_Len, D_Head)\n",
    "        v = v.view(intermim_shape).transpose(1, 2)  # (Batch_Size, N_Heads, Seq_Len, D_Head)\n",
    "\n",
    "        weight = torch.matmul(q, k.transpose(-2, -1))  # (Batch_Size, N_Heads, Seq_Len, Seq_Len)\n",
    "        \n",
    "        if causal_mask:\n",
    "            # mask where the upper triangle (above the main diagonal) is made up of 1\n",
    "            mask = torch.ones_like(weight).triu(1)\n",
    "            weight = weight.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        weight = weight / math.sqrt(self.d_head)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        output = torch.matmul(weight, v)  # (Batch_Size, N_Heads, Seq_Len, D_Head)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, N_Heads, D_Head)\n",
    "        output = output.transpose(1, 2)\n",
    "\n",
    "        output = output.contiguous().view(input_shape)  # (Batch_Size, Seq_Len, D_Embed)\n",
    "        output = self.out_proj(output)  # (Batch_Size, Seq_Len, D_Embed)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-diffusion-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
